{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf7c5c5-37ca-4818-b5e2-1a95e17a2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import copy\n",
    "# import google.generativeai as genai\n",
    "from google import genai\n",
    "\n",
    "# load the API key, use your own API key\n",
    "with open('../API_KEY.txt', 'r') as f:\n",
    "    key = f.read()\n",
    "\n",
    "client = genai.Client(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf089ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['adaptive'], ['less is more'], ['hierarchical'], ['scaling'], ['in-the-wild'], ['self-refine'], ['look-ahead'], ['hindsight'], ['memory'], ['self-'], ['rethink'], ['weak to strong'], ['granularity'], ['in-context learning'], ['reference free'], ['grokking']]\n",
      "[['agent'], ['planning'], ['RAG'], ['safety'], ['calibration'], ['reasoning'], ['persuasion'], ['debate']]\n",
      "[['mamba'], ['RL'], ['linear models'], ['KV'], ['quantization'], ['optimal transport'], ['diffusion'], ['self-attention'], ['self-supervision']]\n",
      "['You are a senior professor in AI and your students propose to do <combination>.Can you refine the title into a good one that can be accepted by top conferences such as ACL/EMNLP/NeurIPS/ICLR, etc. and list the potential abstract, introduction outline, and section names? \\n\\n Requirements: 1. do not hallucinate, 2. do not use any existing paper names in your pretraining data. 3. make sure the title and outline is with an outstanding paper quality so that your student can be happy and successfully graduate.']\n",
      "['A1, B1, C1', 'Comparing C1 and C2 in B1 with A1']\n"
     ]
    }
   ],
   "source": [
    "# import the concept discs: A.json, B.json, C.json and prompt.json in with a helper function\n",
    "\n",
    "import json\n",
    "\n",
    "def import_concept_discs(path):\n",
    "    # research theme\n",
    "    with open(path + 'A.json', 'r') as f:\n",
    "        As = json.load(f)\n",
    "\n",
    "    # research domain\n",
    "    with open(path + 'B.json', 'r') as f:\n",
    "        Bs = json.load(f)\n",
    "\n",
    "    # research architecture\n",
    "    with open(path + 'C.json', 'r') as f:\n",
    "        Cs = json.load(f)\n",
    "\n",
    "    with open(path + 'prompt.json', 'r') as f:\n",
    "        prompts = json.load(f)\n",
    "\n",
    "    with open(path + 'temp.json', 'r') as f:\n",
    "        temps = json.load(f)\n",
    "\n",
    "    return As, Bs, Cs, prompts, temps\n",
    "\n",
    "path = './resource/seed/'\n",
    "As, Bs, Cs, prompts, temps = import_concept_discs(path)\n",
    "\n",
    "print(As)\n",
    "print(Bs)\n",
    "print(Cs)\n",
    "print(prompts)\n",
    "print(temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "808bcaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "\n",
      "['Paris\\n', 'Paris\\n']\n"
     ]
    }
   ],
   "source": [
    "# load the gemini client and generate content\n",
    "\n",
    "def gen_with_gemini(client, prompt):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-pro\",\n",
    "        contents=prompt,\n",
    "        config=genai.types.GenerateContentConfig(\n",
    "            candidate_count=2,\n",
    "            max_output_tokens=1600,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(response.candidates[0].content.parts[0].text)\n",
    "    candidates = [cand.content.parts[0].text for cand in response.candidates]\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# trial run with a prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = gen_with_gemini(client, prompt)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined concepts in a template:  less is more, agent, mamba\n",
      "Let's refine this title and outline to something more conference-ready.  The core idea of \"less is more\" with agents like Mamba needs to be unpacked into a concrete contribution.  We'll focus on efficiency and performance.\n",
      "\n",
      "**Title Options (pick one based on the final angle of the paper):**\n",
      "\n",
      "1. **Sparse Activation Agents: Achieving Efficient Performance with MAMBA** (Focuses on the mechanism)\n",
      "2. **MAMBA:  A Minimalist Agent Architecture for Enhanced Sample Efficiency** (Focuses on efficiency)\n",
      "3. **Towards Efficient Generalization:  Exploring Sparse Activation Policies in Reinforcement Learning** (Broader, focuses on the overall goal)\n",
      "\n",
      "\n",
      "**Abstract (Example using Title 1):**\n",
      "\n",
      "Large language models (LLMs) have demonstrated impressive capabilities as agents, but their computational cost can be prohibitive. We introduce MAMBA (Minimally Activated Model-Based Agent), a novel agent architecture designed to achieve high performance with significantly reduced computational overhead. MAMBA leverages sparse activation techniques, selectively engaging specific components of the LLM based on the current state and task context.  We evaluate MAMBA on a suite of challenging benchmarks [mention benchmarks], demonstrating comparable or superior performance to fully activated LLM agents while requiring substantially fewer computations.  Our findings suggest that sparse activation offers a promising pathway towards developing more efficient and scalable intelligent agents.\n",
      "\n",
      "\n",
      "**Introduction Outline:**\n",
      "\n",
      "1. **Motivation:**  The growing computational cost of LLM-based agents limits their deployment and scalability.  Highlight the need for more efficient agent architectures.\n",
      "2. **Problem Statement:**  Formally define the challenge of achieving high performance with reduced computation in the context of LLM agents.\n",
      "3. **Proposed Approach:** Introduce MAMBA and its core principles, emphasizing the sparse activation mechanism. Briefly describe how it differs from existing approaches.  (e.g., gating, pruning).\n",
      "4. **Key Contributions:**  Summarize the main contributions of the paper (e.g., novel architecture, improved efficiency, strong empirical results).\n",
      "5. **Paper Organization:**  Outline the structure of the paper.\n",
      "\n",
      "\n",
      "\n",
      "**Potential Section Names:**\n",
      "\n",
      "1. **Related Work:**  Discuss existing work on efficient LLM deployment, agent architectures, and sparsity techniques.  (e.g., Mixture-of-Experts,  Adaptive Computation Time)\n",
      "2. **MAMBA Architecture:** Detailed explanation of the MAMBA agent, including the sparse activation mechanism, state representation, action selection, and learning process.  Include diagrams and pseudocode.\n",
      "3. **Experimental Setup:** Describe the benchmarks used for evaluation, including details on the environment, reward function, and baselines.\n",
      "4. **Results:** Present the empirical results, comparing MAMBA's performance and efficiency to baseline agents.  Include tables, graphs, and ablation studies to analyze the impact of different components.\n",
      "5. **Analysis and Discussion:**  Interpret the results, discuss the limitations of the proposed approach, and highlight potential future directions.  (e.g., different sparsity patterns, dynamic adaptation)\n",
      "6. **Conclusion:**  Summarize the key findings and reiterate the contributions of the paper.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Key Considerations for Students:**\n",
      "\n",
      "* **Strong Baselines:**  Choose strong and relevant baselines for comparison.\n",
      "* **Comprehensive Evaluation:**  Evaluate on diverse tasks to demonstrate the generality of the approach.\n",
      "* **Ablation Studies:**  Conduct thorough ablation studies to analyze the contribution of each component of MAMBA.\n",
      "* **Clear Writing:**  Present the ideas clearly and concisely.\n",
      "\n",
      "\n",
      "By following this refined structure and addressing these key considerations, your students will have a much stronger foundation for a successful publication. Remember to adapt the specifics based on the evolving research.\n",
      "\n",
      "['Let\\'s refine this title and outline to something more conference-ready.  The core idea of \"less is more\" with agents like Mamba needs to be unpacked into a concrete contribution.  We\\'ll focus on efficiency and performance.\\n\\n**Title Options (pick one based on the final angle of the paper):**\\n\\n1. **Sparse Activation Agents: Achieving Efficient Performance with MAMBA** (Focuses on the mechanism)\\n2. **MAMBA:  A Minimalist Agent Architecture for Enhanced Sample Efficiency** (Focuses on efficiency)\\n3. **Towards Efficient Generalization:  Exploring Sparse Activation Policies in Reinforcement Learning** (Broader, focuses on the overall goal)\\n\\n\\n**Abstract (Example using Title 1):**\\n\\nLarge language models (LLMs) have demonstrated impressive capabilities as agents, but their computational cost can be prohibitive. We introduce MAMBA (Minimally Activated Model-Based Agent), a novel agent architecture designed to achieve high performance with significantly reduced computational overhead. MAMBA leverages sparse activation techniques, selectively engaging specific components of the LLM based on the current state and task context.  We evaluate MAMBA on a suite of challenging benchmarks [mention benchmarks], demonstrating comparable or superior performance to fully activated LLM agents while requiring substantially fewer computations.  Our findings suggest that sparse activation offers a promising pathway towards developing more efficient and scalable intelligent agents.\\n\\n\\n**Introduction Outline:**\\n\\n1. **Motivation:**  The growing computational cost of LLM-based agents limits their deployment and scalability.  Highlight the need for more efficient agent architectures.\\n2. **Problem Statement:**  Formally define the challenge of achieving high performance with reduced computation in the context of LLM agents.\\n3. **Proposed Approach:** Introduce MAMBA and its core principles, emphasizing the sparse activation mechanism. Briefly describe how it differs from existing approaches.  (e.g., gating, pruning).\\n4. **Key Contributions:**  Summarize the main contributions of the paper (e.g., novel architecture, improved efficiency, strong empirical results).\\n5. **Paper Organization:**  Outline the structure of the paper.\\n\\n\\n\\n**Potential Section Names:**\\n\\n1. **Related Work:**  Discuss existing work on efficient LLM deployment, agent architectures, and sparsity techniques.  (e.g., Mixture-of-Experts,  Adaptive Computation Time)\\n2. **MAMBA Architecture:** Detailed explanation of the MAMBA agent, including the sparse activation mechanism, state representation, action selection, and learning process.  Include diagrams and pseudocode.\\n3. **Experimental Setup:** Describe the benchmarks used for evaluation, including details on the environment, reward function, and baselines.\\n4. **Results:** Present the empirical results, comparing MAMBA\\'s performance and efficiency to baseline agents.  Include tables, graphs, and ablation studies to analyze the impact of different components.\\n5. **Analysis and Discussion:**  Interpret the results, discuss the limitations of the proposed approach, and highlight potential future directions.  (e.g., different sparsity patterns, dynamic adaptation)\\n6. **Conclusion:**  Summarize the key findings and reiterate the contributions of the paper.\\n\\n\\n\\n\\n**Key Considerations for Students:**\\n\\n* **Strong Baselines:**  Choose strong and relevant baselines for comparison.\\n* **Comprehensive Evaluation:**  Evaluate on diverse tasks to demonstrate the generality of the approach.\\n* **Ablation Studies:**  Conduct thorough ablation studies to analyze the contribution of each component of MAMBA.\\n* **Clear Writing:**  Present the ideas clearly and concisely.\\n\\n\\nBy following this refined structure and addressing these key considerations, your students will have a much stronger foundation for a successful publication. Remember to adapt the specifics based on the evolving research.\\n', '## Title Options:\\n\\n1. **Sparse Activation for Efficient and Robust Language Modeling: The MAMBA Agent** (Focuses on efficiency and robustness)\\n2. **MAMBA:  Minimally Activated Modules for Better Agent Performance in Language Tasks** (Highlights the core idea of minimal activation)\\n3. **Towards Efficient Language Agents: Exploring Minimal Module Activation with MAMBA** (More exploratory, suitable if the work is preliminary but promising)\\n4. **Less is More in Language Agents: Achieving Strong Performance with Sparse Module Activation (MAMBA)** (Direct and emphasizes the \"less is more\" concept)\\n\\n\\n## Potential Abstract:\\n\\nLarge language models (LLMs) achieve impressive performance but suffer from high computational costs, hindering their deployment in resource-constrained environments. We propose MAMBA (Minimally Activated Modules for Better Agent performance), a novel approach for training and deploying efficient language agents. MAMBA leverages sparse module activation, where only a subset of the model\\'s parameters are activated for a given input, significantly reducing computational overhead.  We introduce a learning strategy that encourages sparsity while maintaining performance across diverse language tasks.  Experiments on [mention specific datasets, e.g., GLUE, SuperGLUE, etc.] demonstrate that MAMBA achieves comparable or superior performance to state-of-the-art models with significantly reduced computational requirements. We further analyze the sparsity patterns learned by MAMBA, providing insights into the underlying mechanisms of efficient language processing.  Our findings suggest that a \"less is more\" approach can lead to both efficient and robust language agents.\\n\\n\\n## Introduction Outline:\\n\\n1. **Motivation:**\\n    *  High computational cost of LLMs and their impact on deployment.\\n    *  Need for efficient and robust language agents.\\n    *  Briefly introduce the concept of \"less is more\" in other fields (e.g., biology, design) and its potential in AI.\\n\\n2. **Problem Definition:**\\n    *  Formalize the problem of minimizing computational cost while maintaining performance.\\n    *  Highlight the challenges of achieving sparsity in large neural networks.\\n\\n3. **Proposed Approach (MAMBA):**\\n    *  High-level overview of MAMBA and its core principles (sparse module activation).\\n    *  Briefly describe the learning strategy for encouraging sparsity.\\n\\n4. **Contributions:**\\n    *  Clearly list the key contributions of the work (e.g., novel architecture, training algorithm, empirical results).\\n\\n5. **Paper Organization:**\\n\\n\\n## Potential Section Names:\\n\\n1. **Introduction**\\n2. **Related Work** (Discuss related work on model compression, pruning, efficient transformers, etc.)\\n3. **MAMBA: Methodology** (Detailed explanation of the proposed approach, including architecture, training algorithm, and sparsity-inducing mechanisms)\\n4. **Experimental Setup** (Datasets, evaluation metrics, baselines, implementation details)\\n5. **Results and Analysis** (Present the experimental results, comparing MAMBA with baselines. Analyze the sparsity patterns and their impact on performance.)\\n6. **Ablation Study** (Investigate the impact of different components of MAMBA)\\n7. **Discussion** (Discuss the limitations of the current work and potential future directions)\\n8. **Conclusion**\\n\\n\\nThis outline and the title suggestions provide a strong foundation for a paper that could be accepted at top AI conferences.  Remember that the quality of the experiments and the novelty of the approach will be crucial for success.  Good luck to your students! \\n']\n"
     ]
    }
   ],
   "source": [
    "# Spinning up the gemini client\n",
    "\n",
    "# safe save the response to folder named ./response/\n",
    "# the response is a json object with the following keys:  combination, prompt, response\n",
    "def save_response(response, folder_name):\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # if the response name exist, add a number to the end\n",
    "    if os.path.exists(folder_name + '/response.txt'):\n",
    "        i = 1\n",
    "        while os.path.exists(folder_name + '/response_' + str(i) + '.txt'):\n",
    "            i += 1\n",
    "        folder_name = folder_name + '/response_' + str(i) + '.txt'\n",
    "    else:\n",
    "        folder_name = folder_name + '/response.txt'\n",
    "\n",
    "    with open(folder_name, 'w') as f:\n",
    "        json.dump(response, f)\n",
    "\n",
    "def main(As, Bs, Cs, prompts, templates, gemini_client):\n",
    "\n",
    "    # random sample one or two words from theme, domain, and architecture\n",
    "    # read the template, count how many Ax, Bx, Cx in the template\n",
    "    # replace the Ax, Bx, Cx with the randomly selected words from the disc\n",
    "    this_template = copy.deepcopy(templates[0])\n",
    "\n",
    "    mapping = {\"A\": As, \"B\": Bs, \"C\": Cs}\n",
    "    for i in range(3):\n",
    "        for keyword in [\"A\", \"B\", \"C\"]:\n",
    "            this_word = keyword + str(i)\n",
    "            if this_word in this_template:\n",
    "                random.seed(2077+i)\n",
    "                chosen = random.choice(mapping[keyword])\n",
    "                this_template = this_template.replace(this_word, random.choice(chosen))\n",
    "\n",
    "    print(\"Combined concepts in a template: \", this_template)\n",
    "\n",
    "    # generate a prompt\n",
    "    prompt = prompts[0].replace(\"<combination>\", this_template)\n",
    "    print(\"Prompt: \", prompt)\n",
    "\n",
    "    # generate a response\n",
    "    response = gen_with_gemini(gemini_client, prompt)\n",
    "\n",
    "    folder_name = \"./response/\"\n",
    "    output_json = {\"combination\": this_template, \"template\": templates[0], \"prompt\": prompt, \"response\": response}\n",
    "    save_response(response, folder_name)\n",
    "\n",
    "    return response\n",
    "\n",
    "response = main(As, Bs, Cs, prompts, temps, client)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21724737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
